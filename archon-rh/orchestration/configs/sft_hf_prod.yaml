seed: 42
model:
  type: gpt2
  vocab_size: 128
  n_layer: 4
  n_head: 4
  n_embd: 128
trainer:
  batch_size: 8
  steps: 2000
  learning_rate: 3e-4
  output_dir: artifacts/checkpoints/sft_hf_prod
  dataset: /data/mathlib_tactics.jsonl
