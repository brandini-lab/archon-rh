# Production GPU training config for Lambda Labs / RunPod / dedicated instances
# Optimized for A100 40GB GPU

seed: 42
model:
  type: gpt2
  vocab_size: 512
  n_layer: 16       # Deep model for production
  n_head: 16
  n_embd: 1024
  
trainer:
  batch_size: 48    # Large batch for A100
  steps: 20000      # Production training
  learning_rate: 3e-4
  warmup_steps: 500
  output_dir: artifacts/checkpoints/production_gpu
  dataset: prover/datasets/sample_data/mini_mathlib.jsonl
  gradient_accumulation_steps: 2  # Effective batch = 96
  mixed_precision: fp16
  save_every: 1000
  eval_every: 500
  
# Optimizer settings
optimizer:
  type: adamw
  weight_decay: 0.01
  betas: [0.9, 0.95]
  
# Learning rate schedule
scheduler:
  type: cosine
  warmup_ratio: 0.05

